---
title: AI Project
allDay: false
startTime: 18:00
endTime: 20:00
date: 2025-11-21
completed:
tags:
  - basic
  - deeplearning
  - CNN
---
___
ðŸ“† 2025-11-21 20:12

ðŸ”— permalink: [[AI]] [[Deep Learning]] [[EMOTION DETECTION]] [[CNN]]
___

# 2025-11-21 AI Project

## What is Deep Learning?
in deep learning, neural networks is the fundamental architecture that is the heart of deep learning. Neural Network is a computational model. Inspired of the way the human brain works, it includes many neurons which are small computational units linked together in layers. Each neuron receives input signals from other neurons, performs calculations, and then combines the output for the next neuron
There are some different architectures in deep learning, so neural network is a name used to refer to all of these architecture in general
## Common architectures in deep learning
1. Single-layer Perceptron
	- 1 input layer linked directly to the output layer
	- In short, the step function will map the result of the weighted sum calculated in the first step to 1 in 2 value (0 or 1) 
	- Apply: binary classification problem, ... 
2. Multi-layer Perceptron
	- input layer connect to output layer through hidden layers 
	- used activation  function which are usually nonlinear functions
3. Convolutional Neural Network (CNN)
	- it is a special type of Neural Network that is often used to process grid-shaped features (images, ... ) by using convolutional calculations  to automatically learn outstanding features or characteristics  
	- CNN looks at images through filters to learn information to help classify and identify objects effectively
	- CNN includes Convolutional layer, Pooling layer, fully Connected layer, Batch Normalization layer (Optional), Dropout layer
	![[Pasted image 20251122015929.png]]
	- Convolutional layer use less parameter than fully connected layer => CNN saves parameter than multi layer perceptron
4. Recurrent Neural Network 
	- it is a type of neural network specially designed to process and model sequential data 
	- Ability to remember the previous state through the Time-Propagation mechanism 
	- Thanks to it  becomes an important architecture in many different problems in the field of LLM, speech recognition or predicting time series 
	- ![[Pasted image 20251122021454.png]]
	- LSTM and GRU are specially architectures which designed to overcome the problem of vanishing gradient and exploding gradient 
5. Transformer 
	- it is a neural network architecture that was first proposed in the famous article: "Attention Is All You Need"
	- it built on an encoder, decoder architecture that includes two main components. The first is encoder responsible for encoding the input and the second is the decoder responsible for decoding the output
	- ![[Pasted image 20251122022902.png]]
	- encoder and decoder created from many layers on top of each other
	- encoder includes 3 main components:
		- The first component is Multi-head Attention which allows each token in the input pay attention to other tokens in the same input sequence 
		- The second component is Feed Forward Network which is neural network consisting of two layers that are applied to each position or token completely independently of each other 
		- ![[Pasted image 20251122023011.png]]
		- The Third Component is Add & Norm. after each self-attention block or Feed Forward Network, we will combine the use of skip connection and layer normalization
	- decoder includes 4 main components:
		- The first is Masked Multi-Head Attention. It is the same as Multi-Head attention of the encoder but it has an additional masked part to be able to hide information of future position so that the model can see the words before we create them 
		- The second is encoder encoder attention (Cross Attention) helps the decoder to pay attention to the hidden states or hidden representations generated by encoder combined with the output information of the encoder with the string being decoded 
		- The third and fourth are Feed Forward Network and Add & Norm 
	6. Generative Adversarial Network (GAN)
		- it is an extremely famous architecture with many applications 
		- The core idea  of GAN is build 2 neural network against each other, one is a generator whose main task is to create fake data and one is a discriminator whose main task is to distinguish between fake data and real data 
# References

